{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f74aa85-58e6-44c1-9016-c169c1ccb6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import necessary libraries\n",
    "!pip install streamlit\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics.pairwise import euclidean_distances, cosine_similarity\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from scipy.stats import ttest_ind\n",
    "from sklearn.impute import SimpleImputer # Import SimpleImputer\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "import streamlit as st\n",
    "\n",
    "# Title of the Streamlit app\n",
    "st.title(\"Amazon Product Data Analysis\")\n",
    "\n",
    "# Load the dataset (using Streamlit's file uploader)\n",
    "uploaded_file = st.file_uploader(\"amazon.csv\", type=\"csv\")\n",
    "if uploaded_file is not None:\n",
    "    df = pd.read_csv(uploaded_file) \n",
    "else:\n",
    "    st.warning(\"Please upload a CSV file to begin.\")\n",
    "    st.stop()  # Stop execution if no file is uploaded\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "#file_path = 'amazon.csv'  # Replace with your file path\n",
    "#df = pd.read_csv(file_path)\n",
    "\n",
    "\n",
    "# Step 1: Handle Missing Values\n",
    "# Check for missing values in critical columns\n",
    "critical_columns = ['product_id', 'actual_price', 'discounted_price', 'rating', 'rating_count', 'category']\n",
    "print(\"Missing values before cleaning:\\n\", df[critical_columns].isnull().sum())\n",
    "\n",
    "\n",
    "# Drop rows with missing values in critical columns\n",
    "df_cleaned = df.dropna(subset=critical_columns)\n",
    "\n",
    "\n",
    "# Step 2: Normalize Numerical Data- # Data Cleaning and Preprocessing\n",
    "# Remove non-numeric characters and convert to float\n",
    "df_cleaned['actual_price'] = df_cleaned['actual_price'].str.replace('₹', '').str.replace(',', '').astype(float)\n",
    "df_cleaned['discounted_price'] = df_cleaned['discounted_price'].str.replace('₹', '').str.replace(',', '').astype(float)\n",
    "df_cleaned['rating'] = pd.to_numeric(df_cleaned['rating'], errors='coerce')\n",
    "df_cleaned['rating_count'] = df_cleaned['rating_count'].str.replace(',', '').astype(float)\n",
    "\n",
    "\n",
    "# Impute missing values in 'rating' column with the median\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "df_cleaned['rating'] = imputer.fit_transform(df_cleaned[['rating']])\n",
    "\n",
    "\n",
    "# Normalize numerical columns\n",
    "scaler = StandardScaler()\n",
    "numerical_columns = ['actual_price', 'discounted_price', 'rating', 'rating_count']\n",
    "df_cleaned[numerical_columns] = scaler.fit_transform(df_cleaned[numerical_columns])\n",
    "\n",
    "\n",
    "# Step 3: Encode Categorical Features\n",
    "# Split categories and take the first as main category\n",
    "df_cleaned['main_category'] = df_cleaned['category'].str.split('|').str[0]\n",
    "\n",
    "\n",
    "# Encode the 'main_category' column\n",
    "encoder = LabelEncoder()\n",
    "df_cleaned['main_category_encoded'] = encoder.fit_transform(df_cleaned['main_category'])\n",
    "\n",
    "\n",
    "# Display the cleaned and normalized dataframe\n",
    "print(\"Cleaned Data Sample:\\n\", df_cleaned.head())\n",
    "\n",
    "\n",
    "# Data Visualization\n",
    "fig, ax = plt.subplots(figsize=(10, 6))  # Create a Matplotlib figure and axes\n",
    "sns.histplot(df_cleaned['rating'], bins=20, kde=True, ax=ax)  # Plot on the axes\n",
    "ax.set_title('Distribution of Ratings')\n",
    "st.pyplot(fig)  # Display the figure using st.pyplot\n",
    "\n",
    "\n",
    "# Feature Selection for Clustering\n",
    "features = ['discounted_price', 'actual_price', 'main_category_encoded', 'rating', 'rating_count']\n",
    "X = df_cleaned[features]\n",
    "\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "# Determine the optimal number of clusters using the Elbow Method\n",
    "inertia = []\n",
    "cluster_range = range(1, 11)\n",
    "for k in cluster_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(X_scaled)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "\n",
    "# Elbow Method for Optimal Number of Clusters\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(cluster_range, inertia, marker='o', linestyle='-', color='blue')\n",
    "ax.set_title('Elbow Method for Optimal Number of Clusters')\n",
    "ax.set_xlabel('Number of Clusters')\n",
    "ax.set_ylabel('Inertia')\n",
    "ax.grid(True)\n",
    "st.pyplot(fig)\n",
    "\n",
    "\n",
    "# Apply K-Means Clustering\n",
    "optimal_clusters = 4  # Select based on the Elbow Method\n",
    "kmeans = KMeans(n_clusters=optimal_clusters, random_state=42)\n",
    "df_cleaned['customer_segment'] = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# Analyze clusters\n",
    "cluster_summary = df_cleaned.groupby('customer_segment')[features].mean()\n",
    "print(cluster_summary)\n",
    "\n",
    "# Visualize clusters\n",
    "# Customer Segments (Scatter Plot)\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "sns.scatterplot(x='discounted_price', y='rating', hue='customer_segment', size='rating_count', data=df_cleaned, ax=ax)\n",
    "ax.set_title('Customer Segments')\n",
    "st.pyplot(fig)\n",
    "\n",
    "# Visualize Customer Segments using PCA for Dimensionality Reduction\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "df_cleaned['PCA1'] = X_pca[:, 0]\n",
    "df_cleaned['PCA2'] = X_pca[:, 1]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "sns.scatterplot(x='PCA1', y='PCA2', hue='customer_segment', data=df_cleaned, palette='viridis', ax=ax)\n",
    "ax.set_title('Customer Segments Visualization using PCA')\n",
    "ax.set_xlabel('PCA Component 1')\n",
    "ax.set_ylabel('PCA Component 2')\n",
    "ax.legend(title='Segment')\n",
    "ax.grid(True)\n",
    "st.pyplot(fig)\n",
    "\n",
    "\n",
    "# Analyze Customer Segments\n",
    "segment_analysis = df_cleaned.groupby('customer_segment')[features].mean()\n",
    "print(\"Customer Segment Analysis:\\n\", segment_analysis)\n",
    "\n",
    "\n",
    "# Visualize Customer Segment Distribution\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "sns.countplot(x='customer_segment', data=df_cleaned, palette='viridis', ax=ax)\n",
    "ax.set_title('Customer Segment Distribution')\n",
    "ax.set_xlabel('Customer Segment')\n",
    "ax.set_ylabel('Count')\n",
    "st.pyplot(fig)\n",
    "\n",
    "\n",
    "# Prepare Data for Association Rule Mining\n",
    "# Group products by transactions\n",
    "transactions = df_cleaned.groupby(['product_id', 'product_name'])['main_category'].apply(list).values.tolist()\n",
    "\n",
    "\n",
    "# One-hot encode the transactions\n",
    "te = TransactionEncoder()\n",
    "te_array = te.fit(transactions).transform(transactions)\n",
    "df_encoded = pd.DataFrame(te_array, columns=te.columns_)\n",
    "\n",
    "\n",
    "# Apply Apriori Algorithm to find frequent itemsets\n",
    "min_support = 0.01  # Adjust this value based on your dataset\n",
    "frequent_itemsets = apriori(df_encoded, min_support=min_support, use_colnames=True)\n",
    "\n",
    "\n",
    "# Generate Association Rules\n",
    "min_confidence = 0.2  # Adjust this value as needed\n",
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=min_confidence)\n",
    "\n",
    "\n",
    "# Sort rules by lift to find the most interesting ones\n",
    "rules = rules.sort_values(by='lift', ascending=False)\n",
    "\n",
    "\n",
    "# Display the top rules\n",
    "print(\"Top Association Rules:\")\n",
    "print(rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']].head(10))\n",
    "\n",
    "\n",
    "# Visualize the Support, Confidence, and Lift of Top Rules\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "scatter = ax.scatter(rules['support'], rules['confidence'], alpha=0.6, c=rules['lift'], cmap='viridis')\n",
    "ax.set_title('Association Rules: Support vs Confidence')\n",
    "ax.set_xlabel('Support')\n",
    "ax.set_ylabel('Confidence')\n",
    "fig.colorbar(scatter, label='Lift')  # Add colorbar\n",
    "ax.grid(True)\n",
    "st.pyplot(fig)\n",
    "\n",
    "# Decision Tree Classification Example\n",
    "X = df_cleaned[numerical_columns]\n",
    "y = df_cleaned['main_category_encoded']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "# Plot Decision Tree\n",
    "fig, ax = plt.subplots(figsize=(20, 10))  # Adjust figsize as needed\n",
    "plot_tree(clf, feature_names=numerical_columns, class_names=encoder.classes_, filled=True, ax=ax)\n",
    "st.pyplot(fig)\n",
    "\n",
    "# Statistical Analysis\n",
    "cat1 = df_cleaned[df_cleaned['main_category_encoded'] == 0]['rating']\n",
    "cat2 = df_cleaned[df_cleaned['main_category_encoded'] == 1]['rating']\n",
    "t_stat, p_val = ttest_ind(cat1, cat2, nan_policy='omit')\n",
    "print(f\"T-test: t-stat={t_stat}, p-value={p_val}\")\n",
    "\n",
    "\n",
    "# Calculate Discount Percentage\n",
    "df_cleaned['discount_percentage'] = ((df_cleaned['actual_price'] - df_cleaned['discounted_price']) / \n",
    "                                     df_cleaned['actual_price']) * 100\n",
    "\n",
    "\n",
    "# Step 1: Histograms and Box Plots for Prices\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "sns.histplot(df_cleaned['actual_price'], bins=30, kde=True, color='blue', ax=axes[0])\n",
    "axes[0].set_title('Distribution of Actual Prices')\n",
    "axes[0].set_xlabel('Actual Price')\n",
    "sns.histplot(df_cleaned['discounted_price'], bins=30, kde=True, color='green', ax=axes[1])\n",
    "axes[1].set_title('Distribution of Discounted Prices')\n",
    "axes[1].set_xlabel('Discounted Price')\n",
    "plt.tight_layout()\n",
    "st.pyplot(fig)\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "sns.boxplot(y=df_cleaned['actual_price'], color='blue', ax=axes[0])\n",
    "axes[0].set_title('Box Plot of Actual Prices')\n",
    "sns.boxplot(y=df_cleaned['discounted_price'], color='green', ax=axes[1])\n",
    "axes[1].set_title('Box Plot of Discounted Prices')\n",
    "plt.tight_layout()\n",
    "st.pyplot(fig)\n",
    "\n",
    "\n",
    "\n",
    "# Step 2: Scatter Plots for Price Relationships\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "sns.scatterplot(x='actual_price', y='discounted_price', data=df_cleaned, alpha=0.6, ax=ax)\n",
    "ax.set_title('Actual Price vs Discounted Price')\n",
    "ax.set_xlabel('Actual Price')\n",
    "ax.set_ylabel('Discounted Price')\n",
    "st.pyplot(fig)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "sns.scatterplot(x='actual_price', y='discount_percentage', data=df_cleaned, alpha=0.6, color='red', ax=ax)\n",
    "ax.set_title('Actual Price vs Discount Percentage')\n",
    "ax.set_xlabel('Actual Price')\n",
    "ax.set_ylabel('Discount Percentage')\n",
    "st.pyplot(fig)\n",
    "\n",
    "\n",
    "# Step 3: Bar Charts for Rating Distribution and Popular Products\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "sns.countplot(x='rating', data=df_cleaned, palette='viridis', ax=ax)\n",
    "ax.set_title('Product Rating Distribution')\n",
    "ax.set_xlabel('Rating')\n",
    "ax.set_ylabel('Count')\n",
    "st.pyplot(fig)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "sns.histplot(df_cleaned['rating_count'], bins=30, kde=True, color='purple', ax=ax)\n",
    "ax.set_title('Distribution of Rating Count')\n",
    "ax.set_xlabel('Rating Count')\n",
    "st.pyplot(fig)\n",
    "\n",
    "\n",
    "# Step 4: Bar Charts/Pie Charts for Category and Product Popularity\n",
    "category_counts = df_cleaned['main_category'].value_counts()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "category_counts.plot(kind='bar', color='orange', ax=ax)\n",
    "ax.set_title('Product Category Distribution')\n",
    "ax.set_xlabel('Category')\n",
    "ax.set_ylabel('Count')\n",
    "st.pyplot(fig)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "category_counts.plot(kind='pie', autopct='%1.1f%%', startangle=90, colors=sns.color_palette('pastel'), ax=ax)\n",
    "ax.set_title('Product Category Distribution (Pie Chart)')\n",
    "ax.set_ylabel('')  # Remove ylabel\n",
    "st.pyplot(fig)\n",
    "\n",
    "\n",
    "# Step 5: Correlation Heatmap\n",
    "correlation_matrix = df_cleaned[['actual_price', 'discounted_price', 'rating', 'rating_count', 'discount_percentage']].corr()\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5, ax=ax)\n",
    "ax.set_title('Correlation Heatmap')\n",
    "st.pyplot(fig)\n",
    "\n",
    "\n",
    "# Drop rows with missing values in user-related columns\n",
    "user_columns = ['user_id', 'user_name', 'review_title', 'review_content', 'rating_count']\n",
    "df_cleaned = df.dropna(subset=user_columns)\n",
    "\n",
    "# Drop rows with NaN rating_count\n",
    "df_cleaned = df_cleaned.dropna(subset=['rating_count'])  \n",
    "\n",
    "# Basic User Behavior Analysis\n",
    "# Top Users by Review Count\n",
    "top_users = df_cleaned['user_name'].value_counts().head(10)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "top_users.plot(kind='bar', color='blue', ax=ax)\n",
    "ax.set_title('Top 10 Users by Review Count')\n",
    "ax.set_xlabel('User Name')\n",
    "ax.set_ylabel('Number of Reviews')\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')  # Rotate x-axis labels\n",
    "ax.grid(True)\n",
    "st.pyplot(fig)\n",
    "\n",
    "\n",
    "# Rating Distribution Analysis\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "sns.histplot(df_cleaned['rating_count'], bins=20, color='green', kde=True, ax=ax)\n",
    "ax.set_title('Distribution of Rating Count')\n",
    "ax.set_xlabel('Rating Count')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.grid(True)\n",
    "st.pyplot(fig)\n",
    "\n",
    "\n",
    "# Sentiment Analysis on Review Content\n",
    "def get_sentiment(text):\n",
    "    analysis = TextBlob(text)\n",
    "    return analysis.sentiment.polarity\n",
    "\n",
    "\n",
    "df_cleaned['sentiment'] = df_cleaned['review_content'].apply(get_sentiment)\n",
    "\n",
    "\n",
    "# Plot Sentiment Distribution\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "sns.histplot(df_cleaned['sentiment'], bins=20, color='purple', kde=True, ax=ax)\n",
    "ax.set_title('Sentiment Distribution of Reviews')\n",
    "ax.set_xlabel('Sentiment Polarity')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.grid(True)\n",
    "st.pyplot(fig)\n",
    "\n",
    "\n",
    "# Word Cloud of Review Content\n",
    "review_text = ' '.join(df_cleaned['review_content'].astype(str).values)\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white', stopwords=STOPWORDS).generate(review_text)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.title('Word Cloud of Review Content')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Topic Modeling using Latent Dirichlet Allocation (LDA)\n",
    "vectorizer = CountVectorizer(max_df=0.9, min_df=2, stop_words='english')\n",
    "dtm = vectorizer.fit_transform(df_cleaned['review_content'].astype(str))\n",
    "lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "lda.fit(dtm)\n",
    "\n",
    "\n",
    "# Display the Top Words for each Topic\n",
    "num_words = 10\n",
    "words = vectorizer.get_feature_names_out()\n",
    "for i, topic in enumerate(lda.components_):\n",
    "    print(f\"\\nTopic {i+1}:\")\n",
    "    print([words[j] for j in topic.argsort()[-num_words:][::-1]])\n",
    "\n",
    "# Word Cloud of Review Content\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.imshow(wordcloud, interpolation='bilinear')\n",
    "ax.set_title('Word Cloud of Review Content')\n",
    "ax.axis('off')\n",
    "st.pyplot(fig)\n",
    "\n",
    "\n",
    "\n",
    "# Customer Segmentation based on Sentiment and Rating Count\n",
    "df_cleaned['sentiment_label'] = pd.cut(df_cleaned['sentiment'], bins=[-1, -0.01, 0.01, 1], labels=['Negative', 'Neutral', 'Positive'])\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "sns.countplot(x='sentiment_label', data=df_cleaned, palette='viridis',dodge=False, ax=ax)\n",
    "ax.set_title('Customer Segments based on Sentiment')\n",
    "ax.set_xlabel('Sentiment')\n",
    "ax.set_ylabel('Number of Reviews')\n",
    "ax.grid(True)\n",
    "st.pyplot(fig)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='sentiment_label', data=df_cleaned, palette='viridis')\n",
    "plt.title('Customer Segments based on Sentiment')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Number of Reviews')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Cross-tabulation of Rating Count and Sentiment\n",
    "rating_sentiment_crosstab = pd.crosstab(df_cleaned['rating_count'], df_cleaned['sentiment_label'])\n",
    "print(\"\\nRating Count vs Sentiment Analysis:\\n\", rating_sentiment_crosstab)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
